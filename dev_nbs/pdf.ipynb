{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffba4f5-38f0-43a3-89e5-36aba0841637",
   "metadata": {},
   "source": [
    "# QA over documents\n",
    "Step-by-step tutorial for making a [question-answering application](https://python.langchain.com/docs/use_cases/question_answering/) over PDF documents with Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35068ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import gradio as gr\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFiles: \n",
    "\n",
    "    def __init__(self, chunk_size: int = 2000, chunck_overlap: int = 100, db_path: str = None):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunck_overlap\n",
    "        self.directory = Path(db_path) / 'index_store' if db_path else Path('..') / 'index_store'\n",
    "\n",
    "    def create_vector_db_from_docs(self, documents: list):\n",
    "        \"\"\"Create a vector database from a list of documents.\"\"\"\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        vectordb =  FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "        vectordb.save_local(self.directory)\n",
    "        return vectordb\n",
    "\n",
    "    def load_pdf_files(self, files: list):\n",
    "        \"\"\"Load PDF files and return their contents.\"\"\"\n",
    "        documents = []\n",
    "        for pdf_path in files:\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "        return documents\n",
    "\n",
    "    def load_local_db(self):\n",
    "        \"\"\"Load the existing vector database if existing.\"\"\"\n",
    "        if self.directory.exists():\n",
    "            return FAISS.load_local(self.directory, OpenAIEmbeddings())\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7442796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomChatbot:\n",
    "    \"\"\"Custom Chatbot class to handle PDF documents and vector database.\"\"\"\n",
    "    \n",
    "    def __init__(self, prompt_template: str, custom_files: CustomFiles):\n",
    "        \"\"\"Initialize CustomChatbot instance.\n",
    "\n",
    "        Args:\n",
    "            prompt_template: The prompt for the QA chain.\n",
    "            db_path: Path to the directory where the vector database is stored.\n",
    "        \"\"\"\n",
    "        self.prompt = PromptTemplate.from_template(prompt_template)\n",
    "        self.custom_files = custom_files\n",
    "        self.chain = None\n",
    "\n",
    "    def set_chain(self, chain):\n",
    "        self.chain = chain\n",
    "\n",
    "    def load_and_save_vector_db(self, documents: list, model_id: str, k_search: int):\n",
    "        \"\"\"Load and save a vector database and initialize a QA chain.\"\"\"\n",
    "        vectordb = self.custom_files.create_vector_db_from_docs(documents)\n",
    "        self.set_chain(self.create_qa_chain(vectordb, model_id, k_search))\n",
    "\n",
    "    def create_qa_chain(self, vectordb, model_id: str, k_search: int):\n",
    "        \"\"\"Create a QA chain based on a vector database and a model identifier.\"\"\"\n",
    "        llm = ChatOpenAI(temperature=0, model_name=model_id)\n",
    "        retriever = vectordb.as_retriever(\n",
    "            search_type=\"similarity\", search_kwargs={\"k\": k_search}\n",
    "        )\n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs={\"prompt\": self.prompt},\n",
    "            return_source_documents=True\n",
    "        )\n",
    "\n",
    "    def upload_files(self, files: list, model_id: str, k_search: int):\n",
    "        \"\"\"Upload files and update the vector database.\"\"\"\n",
    "        documents = self.custom_files.load_pdf_files([str(fn.name) for fn in files])\n",
    "        self.load_and_save_vector_db(documents, model_id, k_search)\n",
    "        return gr.Textbox.update(placeholder='Type your message here', lines=1, interactive=True)\n",
    "\n",
    "    def get_vector_db(self, model_id: str, k_search: int):\n",
    "        \"\"\"Get the existing vector database and initialize a QA chain if existing.\"\"\"\n",
    "        placeholder_text = 'Document store does not exist, please upload document(s).'\n",
    "        interactive = False\n",
    "        vector_db = self.custom_files.load_local_db()\n",
    "        if vector_db: \n",
    "            self.set_chain(self.create_qa_chain(vector_db, model_id, k_search))\n",
    "            placeholder_text = 'Type your message here'\n",
    "            interactive = True\n",
    "            \n",
    "        return gr.Textbox.update(placeholder=placeholder_text, lines=1, interactive=interactive)\n",
    "\n",
    "    @staticmethod\n",
    "    def user(user_message: str, history: list):\n",
    "        \"\"\"Handle user messages.\"\"\"\n",
    "        return \"\", history + [[user_message, None]]\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_source_and_page(documents: list):\n",
    "        \"\"\"Extract the source and page number from the source documents used to generate answers.\"\"\"\n",
    "        results = {}\n",
    "        for document in documents:\n",
    "            source = Path(document.metadata['source']).stem\n",
    "            page = document.metadata['page'] + 1\n",
    "            if source in results:\n",
    "                results[source].append(page)\n",
    "            else:\n",
    "                results[source] = [page]\n",
    "        return results\n",
    "\n",
    "    def build_response_message(self, result: str, source_and_page: dict):\n",
    "        \"\"\"Build a response message with information about the source and page(s).\"\"\"\n",
    "        response_message = f\"{result}\\n\\nInformasjonen er hentet fra: \\n\"\n",
    "        source_and_page_str = ''.join(\n",
    "            [f'- Kilde: {source}, Side {sorted(pages)}\\n' for source, pages in source_and_page.items()]\n",
    "        )\n",
    "        return f\"{response_message} {source_and_page_str}\"\n",
    "\n",
    "    def generate_response(self, history: list):\n",
    "        \"\"\"Generate a response for the user based on their query and update the conversation history.\"\"\"\n",
    "        try:\n",
    "            response = self.chain({\"query\": history[-1][0]})\n",
    "            source_and_page = self.extract_source_and_page(response[\"source_documents\"])\n",
    "            response_message = self.build_response_message(response['result'], source_and_page)\n",
    "        except Exception as e:\n",
    "            response_message = (\n",
    "                \"Beklager, tekstens lengde overstiger modellens maksimale kapasitet for kontekstlengde.\"\n",
    "                \"Vennligst reduser k-search eller bytt til en modell med st√∏rre kontekstlengde.\"\n",
    "                )\n",
    "\n",
    "        history[-1][1] = \"\"\n",
    "\n",
    "        for char in response_message:\n",
    "            history[-1][1] += char\n",
    "            yield history\n",
    "            time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a knowledge bot. Use the following pieces of context to answer the question at the end. \n",
    "Provide a detailed answer if possible.\n",
    "Write at the end that the user is responsible for checking that the information provided is correct (in Norwegian).\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful answer in Norwegian:\n",
    "\"\"\"\n",
    "\n",
    "# Get OpenAI API key, see the readme for more info\n",
    "os.environ['OPENAI_API_KEY'] = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "# Create a ChatBot and CustomFiles instances\n",
    "custom_files = CustomFiles()\n",
    "chatbot_obj = CustomChatbot(template, custom_files)\n",
    "\n",
    "# Define UI elements and interactions\n",
    "with gr.Blocks() as blocks:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            model_dropdown = gr.Dropdown(\n",
    "                choices=[\"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\", \"gpt-4\"],\n",
    "                value='gpt-3.5-turbo',\n",
    "                interactive=True,\n",
    "                label=\"Select model\",\n",
    "                info=\"Click on the üìÅ or üóÑÔ∏è button if you want to change the model during the conversation.\"\n",
    "            )\n",
    "        with gr.Column(scale=0.05):\n",
    "            k_search = gr.Number(\n",
    "                minimum=1,\n",
    "                maximum=5,\n",
    "                value=3,\n",
    "                step=1,\n",
    "                precision=0,\n",
    "                label='K search',\n",
    "                interactive=True,\n",
    "                info=\"Number of search results to retrieve from vector database.\"\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        chat_ui = gr.Chatbot(elem_id=\"chatbot\", height=300)\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_text = gr.Textbox(\n",
    "                show_label=False,\n",
    "                lines=2,\n",
    "                placeholder=(\"Please upload document(s) or use existing document store to use the chatbot\"),\n",
    "                interactive=False,\n",
    "                container=False,\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=0.1):\n",
    "            db_button = gr.Button(\"üóÑÔ∏è Use document store\")\n",
    "            db_button.click(\n",
    "                chatbot_obj.get_vector_db,\n",
    "                [model_dropdown, k_search],\n",
    "                outputs=[input_text]\n",
    "            )\n",
    "        with gr.Column(scale=0.1):\n",
    "            upload_button = gr.UploadButton(\n",
    "                \"üìÅ Upload PDF document(s)\",\n",
    "                file_types=[\".pdf\"],\n",
    "                file_count=\"multiple\"\n",
    "            )\n",
    "            upload_button.upload(\n",
    "                chatbot_obj.upload_files,\n",
    "                [upload_button, model_dropdown, k_search],\n",
    "                outputs=[input_text]\n",
    "            )\n",
    "\n",
    "        with gr.Column(scale=0.1):\n",
    "            clear_button = gr.Button(\"üóëÔ∏è Clear chat\")\n",
    "            clear_button.click(lambda: None, None, chat_ui, queue=False)\n",
    "\n",
    "    input_text.submit(\n",
    "        chatbot_obj.user,\n",
    "        [input_text, chat_ui],\n",
    "        [input_text, chat_ui]\n",
    "    ).then(\n",
    "        chatbot_obj.generate_response,\n",
    "        inputs=chat_ui,\n",
    "        outputs=chat_ui\n",
    "    )\n",
    "\n",
    "# Launch UI\n",
    "blocks.queue()\n",
    "blocks.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
