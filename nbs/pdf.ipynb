{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffba4f5-38f0-43a3-89e5-36aba0841637",
   "metadata": {},
   "source": [
    "# QA over documents\n",
    "Step-by-step tutorial for making a [question-answering application](https://python.langchain.com/docs/use_cases/question_answering/) over PDF documents with Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35068ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\satkal\\AppData\\Local\\miniconda3\\envs\\fastai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import gradio as gr\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef06c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomChatbot:\n",
    "    \"\"\"Custom Chatbot class to handle PDF documents and vector database.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, prompt_template: str, db_path: str = None,\n",
    "                 chunk_size: int = 2000, chunk_overlap: int = 100):\n",
    "        \"\"\"Initialize CustomChatbot instance.\n",
    "\n",
    "        Args:\n",
    "            prompt_template: The prompt for the QA chain.\n",
    "            db_path: Path to the directory where the vector database is stored.\n",
    "            chunk_size: Size of chunks for text splitting. \n",
    "            chunk_overlap: Overlap between chunks for text splitting. \n",
    "        \"\"\"\n",
    "        os.environ['OPENAI_API_KEY'] = api_key\n",
    "        self.prompt = PromptTemplate.from_template(prompt_template)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.chain = None\n",
    "        self.directory = Path(db_path) / 'index_store' if db_path else Path('..') / 'index_store'\n",
    "\n",
    "\n",
    "    def set_chain(self, chain):\n",
    "        self.chain = chain\n",
    "\n",
    "    def load_and_save_vector_db(self, documents: list, model_id: str, k_search: int):\n",
    "        \"\"\"Load and save a vector database and initialize a QA chain.\"\"\"\n",
    "        vectordb = self.create_vector_db_from_docs(documents)\n",
    "        vectordb.save_local(self.directory)\n",
    "        self.set_chain(self.create_qa_chain(vectordb, model_id, k_search))\n",
    "\n",
    "    def create_vector_db_from_docs(self, documents: list):\n",
    "        \"\"\"Create a vector database from a list of documents.\"\"\"\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        return FAISS.from_documents(texts, OpenAIEmbeddings())\n",
    "\n",
    "    def load_pdf_files(self, files: list):\n",
    "        \"\"\"Load PDF files and return their contents.\"\"\"\n",
    "        documents = []\n",
    "        for pdf_path in files:\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "        return documents\n",
    "\n",
    "    def create_qa_chain(self, vectordb, model_id: str, k_search: int):\n",
    "        \"\"\"Create a QA chain based on a vector database and a model identifier.\"\"\"\n",
    "        llm = ChatOpenAI(temperature=0, model_name=model_id)\n",
    "        retriever = vectordb.as_retriever(\n",
    "            search_type=\"similarity\", search_kwargs={\"k\": k_search}\n",
    "        )\n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs={\"prompt\": self.prompt},\n",
    "            return_source_documents=True\n",
    "        )\n",
    "\n",
    "    def upload_files(self, files: list, model_id: str, k_search: int):\n",
    "        \"\"\"Upload files and update the vector database.\"\"\"\n",
    "        documents = self.load_pdf_files([str(fn.name) for fn in files])\n",
    "        self.load_and_save_vector_db(documents, model_id, k_search)\n",
    "        return gr.Textbox.update(placeholder='Type your message here', lines=1, interactive=True)\n",
    "\n",
    "    def get_vector_db(self, model_id: str, k_search: int):\n",
    "        \"\"\"Get the existing vector database and initialize a QA chain if existing.\"\"\"\n",
    "        if not (self.directory).exists():\n",
    "            return gr.Textbox.update(\n",
    "                placeholder='Document store does not exist, please upload document(s).',\n",
    "                lines=1,\n",
    "                interactive=False\n",
    "            )\n",
    "        vectordb = FAISS.load_local(self.directory, OpenAIEmbeddings())\n",
    "        self.set_chain(self.create_qa_chain(vectordb, model_id, k_search))\n",
    "        return gr.Textbox.update(placeholder='Type your message here', lines=1, interactive=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def user(user_message: str, history: list):\n",
    "        \"\"\"Handle user messages.\"\"\"\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_source_and_page(documents: list):\n",
    "        \"\"\"Extract the source and page number from the source documents used to generate answers.\"\"\"\n",
    "        results = {}\n",
    "        for document in documents:\n",
    "            source = Path(document.metadata['source']).stem\n",
    "            page = document.metadata['page'] + 1\n",
    "            if source in results:\n",
    "                results[source].append(page)\n",
    "            else:\n",
    "                results[source] = [page]\n",
    "        return results\n",
    "\n",
    "    def build_response_message(self, result: str, source_and_page: dict):\n",
    "        \"\"\"Build a response message with information about the source and page(s).\"\"\"\n",
    "        response_message = f\"{result}\\n\\nInformasjonen er hentet fra: \\n\"\n",
    "        source_and_page_str = ''.join(\n",
    "            [f'- Kilde: {source}, Side {sorted(pages)}\\n' for source, pages in source_and_page.items()]\n",
    "        )\n",
    "        return f\"{response_message} {source_and_page_str}\"\n",
    "\n",
    "    def generate_response(self, history: list):\n",
    "        \"\"\"Generate a response for the user based on their query and update the conversation history.\"\"\"\n",
    "        try:\n",
    "            response = self.chain({\"query\": history[-1][0]})\n",
    "            source_and_page = self.extract_source_and_page(response[\"source_documents\"])\n",
    "            response_message = self.build_response_message(response['result'], source_and_page)\n",
    "        except Exception as e:\n",
    "            response_message = (\n",
    "                \"Beklager, tekstens lengde overstiger modellens maksimale kapasitet for kontekstlengde.\"\n",
    "                \"Vennligst reduser k-search eller bytt til en modell med st√∏rre kontekstlengde.\"\n",
    "                )\n",
    "\n",
    "        history[-1][1] = \"\"\n",
    "\n",
    "        for char in response_message:\n",
    "            history[-1][1] += char\n",
    "            yield history\n",
    "            time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1bc1a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satkal\\AppData\\Local\\Temp\\ipykernel_13804\\2256700989.py:47: GradioDeprecationWarning: `height` is deprecated in `Interface()`, please use it within `launch()` instead.\n",
      "  input_text = gr.Textbox(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You are a knowledge bot. Use the following pieces of context to answer the question at the end. \n",
    "Provide a detailed answer if possible.\n",
    "Write at the end that the user is responsible for checking that the information provided is correct (in Norwegian).\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful answer in Norwegian:\n",
    "\"\"\"\n",
    "# Get OpenAI API key, see the readme for more info\n",
    "api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "# Create a ChatBot instance\n",
    "chatbot_obj = CustomChatbot(api_key, template)\n",
    "\n",
    "# Define UI elements and interactions\n",
    "with gr.Blocks() as blocks:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            model_dropdown = gr.Dropdown(\n",
    "                choices=[\"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\", \"gpt-4\"],\n",
    "                value='gpt-3.5-turbo',\n",
    "                interactive=True,\n",
    "                label=\"Select model\",\n",
    "                info=\"Click on the üìÅ or üóÑÔ∏è button if you want to change the model during the conversation.\"\n",
    "            )\n",
    "        with gr.Column(scale=0.05):\n",
    "            k_search = gr.Number(\n",
    "                minimum=1,\n",
    "                maximum=5,\n",
    "                value=3,\n",
    "                step=1,\n",
    "                precision=0,\n",
    "                label='K search',\n",
    "                interactive=True,\n",
    "                info=\"Number of search results to retrieve from vector database.\"\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        chat_ui = gr.Chatbot(elem_id=\"chatbot\", height=300)\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_text = gr.Textbox(\n",
    "                show_label=False,\n",
    "                lines=2,\n",
    "                placeholder=(\"Please upload document(s) or use existing document store to use the chatbot\"),\n",
    "                interactive=False,\n",
    "                container=False,\n",
    "                height=100\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=0.1):\n",
    "            db_button = gr.Button(\"üóÑÔ∏è Use document store\")\n",
    "            db_button.click(\n",
    "                chatbot_obj.get_vector_db,\n",
    "                [model_dropdown, k_search],\n",
    "                outputs=[input_text]\n",
    "            )\n",
    "        with gr.Column(scale=0.1):\n",
    "            upload_button = gr.UploadButton(\n",
    "                \"üìÅ Upload PDF document(s)\",\n",
    "                file_types=[\".pdf\"],\n",
    "                file_count=\"multiple\"\n",
    "            )\n",
    "            upload_button.upload(\n",
    "                chatbot_obj.upload_files,\n",
    "                [upload_button, model_dropdown, k_search],\n",
    "                outputs=[input_text]\n",
    "            )\n",
    "\n",
    "        with gr.Column(scale=0.1):\n",
    "            clear_button = gr.Button(\"üóëÔ∏è Clear chat\")\n",
    "            clear_button.click(lambda: None, None, chat_ui, queue=False)\n",
    "\n",
    "    input_text.submit(\n",
    "        chatbot_obj.user,\n",
    "        [input_text, chat_ui],\n",
    "        [input_text, chat_ui]\n",
    "    ).then(\n",
    "        chatbot_obj.generate_response,\n",
    "        inputs=chat_ui,\n",
    "        outputs=chat_ui\n",
    "    )\n",
    "\n",
    "# Launch UI\n",
    "blocks.queue()\n",
    "blocks.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8790167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
